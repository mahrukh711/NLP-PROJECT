{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Coffee Sentiment Analysis\n",
    "\n",
    "This notebook implements a sentiment analysis system for coffee reviews using Natural Language Processing (NLP) techniques. It includes:\n",
    "1. Web scraping from coffeereview.com\n",
    "2. Text preprocessing and NLP\n",
    "3. Sentiment analysis and visualization\n",
    "\n",
    "## Setup and Dependencies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Install required packages\n",
    "!pip install beautifulsoup4 nltk numpy pandas plotly requests scikit-learn streamlit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Import required libraries\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.sentiment import vader\n",
    "import nltk\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import classification_report\n",
    "import plotly.express as px\n",
    "import plotly.graph_objects as go\n",
    "import time\n",
    "import pickle"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Web Scraping\n",
    "\n",
    "First, we'll implement the web scraping functionality to collect coffee reviews."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def scrape_coffee_reviews(num_pages=5):\n",
    "    reviews = []\n",
    "    base_url = \"https://www.coffeereview.com/review/page/{}/\"\n",
    "    \n",
    "    for page in range(1, num_pages + 1):\n",
    "        try:\n",
    "            response = requests.get(base_url.format(page))\n",
    "            soup = BeautifulSoup(response.content, 'html.parser')\n",
    "            \n",
    "            # Find all review containers\n",
    "            review_elements = soup.find_all('article', class_='review')\n",
    "            \n",
    "            for review in review_elements:\n",
    "                title = review.find('h2').text.strip() if review.find('h2') else ''\n",
    "                rating = review.find('div', class_='rating').text.strip() if review.find('div', class_='rating') else ''\n",
    "                text = review.find('div', class_='entry-content').text.strip() if review.find('div', class_='entry-content') else ''\n",
    "                \n",
    "                reviews.append({\n",
    "                    'title': title,\n",
    "                    'rating': rating,\n",
    "                    'text': text\n",
    "                })\n",
    "            \n",
    "            print(f\"Scraped page {page}\")\n",
    "            time.sleep(1)  # Be respectful with scraping\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(f\"Error on page {page}: {str(e)}\")\n",
    "            continue\n",
    "    \n",
    "    return pd.DataFrame(reviews)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. NLP Processing\n",
    "\n",
    "Now we'll implement the sentiment analysis and NLP processing functionality."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Download required NLTK data\n",
    "nltk.download('punkt')\n",
    "nltk.download('stopwords')\n",
    "nltk.download('vader_lexicon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "class CoffeeSentimentAnalyzer:\n",
    "    def __init__(self):\n",
    "        self.stemmer = PorterStemmer()\n",
    "        self.stop_words = set(stopwords.words('english'))\n",
    "        self.sia = vader.SentimentIntensityAnalyzer()\n",
    "        self.vectorizer = TfidfVectorizer()\n",
    "        self.model = RandomForestClassifier()\n",
    "        \n",
    "    def preprocess_text(self, text):\n",
    "        # Tokenization\n",
    "        tokens = word_tokenize(text.lower())\n",
    "        \n",
    "        # Remove stopwords and stem\n",
    "        tokens = [self.stemmer.stem(token) for token in tokens \n",
    "                 if token.isalnum() and token not in self.stop_words]\n",
    "        \n",
    "        return ' '.join(tokens)\n",
    "    \n",
    "    def extract_features(self, text):\n",
    "        # Basic features\n",
    "        features = {}\n",
    "        features['text_length'] = len(text)\n",
    "        features['word_count'] = len(text.split())\n",
    "        \n",
    "        # VADER sentiment scores\n",
    "        sentiment_scores = self.sia.polarity_scores(text)\n",
    "        features.update(sentiment_scores)\n",
    "        \n",
    "        return features\n",
    "    \n",
    "    def prepare_data(self, df):\n",
    "        # Preprocess text\n",
    "        df['processed_text'] = df['text'].apply(self.preprocess_text)\n",
    "        \n",
    "        # Extract features\n",
    "        features_df = pd.DataFrame([self.extract_features(text) \n",
    "                                  for text in df['text']])\n",
    "        \n",
    "        # TF-IDF\n",
    "        tfidf_matrix = self.vectorizer.fit_transform(df['processed_text'])\n",
    "        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), \n",
    "                               columns=self.vectorizer.get_feature_names_out())\n",
    "        \n",
    "        # Combine all features\n",
    "        X = pd.concat([features_df, tfidf_df], axis=1)\n",
    "        \n",
    "        # Create labels (assuming ratings are 0-100)\n",
    "        df['sentiment'] = pd.cut(pd.to_numeric(df['rating'].str.replace('[^\\d.]', ''), \n",
    "                                             errors='coerce'),\n",
    "                               bins=[0, 60, 80, 100],\n",
    "                               labels=['negative', 'neutral', 'positive'])\n",
    "        \n",
    "        return X, df['sentiment']\n",
    "    \n",
    "    def train(self, X, y):\n",
    "        self.model.fit(X, y)\n",
    "        \n",
    "    def predict(self, text):\n",
    "        processed_text = self.preprocess_text(text)\n",
    "        features = self.extract_features(text)\n",
    "        features_df = pd.DataFrame([features])\n",
    "        \n",
    "        tfidf_matrix = self.vectorizer.transform([processed_text])\n",
    "        tfidf_df = pd.DataFrame(tfidf_matrix.toarray(), \n",
    "                               columns=self.vectorizer.get_feature_names_out())\n",
    "        \n",
    "        X = pd.concat([features_df, tfidf_df], axis=1)\n",
    "        return self.model.predict(X)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Collection and Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Scrape reviews\n",
    "print(\"Scraping coffee reviews...\")\n",
    "df = scrape_coffee_reviews()\n",
    "df.to_csv(\"coffee_reviews.csv\", index=False)\n",
    "print(f\"Scraped {len(df)} reviews\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Initialize and train the model\n",
    "analyzer = CoffeeSentimentAnalyzer()\n",
    "X, y = analyzer.prepare_data(df)\n",
    "analyzer.train(X, y)\n",
    "print(\"Model trained successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Analysis and Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Sentiment Distribution\n",
    "sentiment_dist = df['sentiment'].value_counts()\n",
    "fig = px.pie(values=sentiment_dist.values, names=sentiment_dist.index, title=\"Review Sentiments\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "# Rating Distribution\n",
    "fig = px.histogram(df, x='rating', title=\"Rating Distribution\")\n",
    "fig.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Interactive Review Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "source": [
    "def analyze_review(text):\n",
    "    sentiment = analyzer.predict(text)\n",
    "    features = analyzer.extract_features(text)\n",
    "    \n",
    "    print(f\"Predicted Sentiment: {sentiment}\")\n",
    "    print(\"\\nFeature Analysis:\")\n",
    "    for feature, value in features.items():\n",
    "        if feature != 'compound':\n",
    "            print(f\"{feature}: {value:.3f}\")\n",
    "\n",
    "# Example usage\n",
    "sample_review = \"This coffee has an amazing aroma with hints of chocolate and a smooth finish.\"\n",
    "analyze_review(sample_review)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}